<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
  /*
      http://www.oschina.net/question/124879_76399
  */
  
  html { font-size: 100%; overflow-y: scroll; -webkit-text-size-adjust: 100%; -ms-text-size-adjust: 100%; }
   
  body{
      color:#444;
      font-family:Georgia, Palatino, 'Palatino Linotype', Times, 'Times New Roman', serif;
      font-size:13px;
      line-height:1.5em;
      padding:1em;
      margin:auto;
      max-width:42em;
      background:#fefefe;
  }
   
  h1, h2, h3, h4, h5, h6 {
      font-weight: bold;
  }
   
  h1 {
      color: #000000;
      font-size: 28px;
  }
   
  h2 {
      border-bottom: 2px solid #CCCCCC;
      color: #000000;
      font-size: 24px;
  }
   
  h3 {
      border-bottom: 2px solid #CCCCCC;
      font-size: 18px;
  }
   
  h4 {
      font-size: 16px;
  }
   
  h5 {
      font-size: 14px;
  }
   
  h6 {
      color: #777777;
      background-color: inherit;
      font-size: 14px;
  }
   
  hr {
      height: 0.2em;
      border: 0;
      color: #CCCCCC;
      background-color: #CCCCCC;
  }
   
  p, blockquote, ul, ol, dl, li, table, pre {
      margin: 15px 0;
  }
   
  p{
      margin:1em 0;
  }
   
  pre { 
      background-color: #F8F8F8;    
      border: 1px solid #CCCCCC;
      border-radius: 3px;
      overflow: auto;
      padding: 5px;
  }
   
  pre code {
      background-color: #F8F8F8;
      border: none;    
      padding: 0;
  }
   
  code {
      font-family: Consolas, Monaco, Andale Mono, monospace;
      background-color:#F8F8F8;
      border: 1px solid #CCCCCC;
      border-radius: 3px;
      padding: 0 0.2em;
      line-height: 1;
  }
   
  pre > code {
      border: 0;
      margin: 0;
      padding: 0;
  }
   
   
  a{ color: #0645ad; text-decoration:none;}
  a:visited{ color: #0b0080; }
  a:hover{ color: #06e; }
  a:active{ color:#faa700; }
  a:focus{ outline: thin dotted; }
  a:hover, a:active{ outline: 0; }
   
  ::-moz-selection{background:rgba(255,255,0,0.3);color:#000}
  ::selection{background:rgba(255,255,0,0.3);color:#000}
   
  a::-moz-selection{background:rgba(255,255,0,0.3);color:#0645ad}
  a::selection{background:rgba(255,255,0,0.3);color:#0645ad}
   
  blockquote{
      color:#666666;
      margin:0;
      padding-left: 3em;
      border-left: 0.5em #EEE solid;
  }
   
  ul, ol { margin: 1em 0; padding: 0 0 0 2em; }
  li p:last-child { margin:0 }
  dd { margin: 0 0 0 2em; }
   
  img { border: 0; -ms-interpolation-mode: bicubic; vertical-align: middle; max-width:100%;}
   
  table { border-collapse: collapse; border-spacing: 0; }
  td { vertical-align: top; }
   
  @media only screen and (min-width: 480px) {
      body{font-size:14px;}
  }
   
  @media only screen and (min-width: 768px) {
      body{font-size:16px;}
  }
  </style>
</head>
<body>
<h1 id="a-basic-nnet-with-sgd-and-backprop">A Basic NNet with SGD and Backprop</h1>
<h2 id="error-term">Error Term</h2>
<p>I used squared error. Namely, En = (y - y_predict)^2</p>
<h2 id="transform-function">Transform Function</h2>
<p>I used the <strong>tanh</strong>.</p>
<h2 id="visual-explanation">Visual Explanation</h2>
<p><img src="nn_pic.png" alt="nn_visual" /></p>
<p>The differences between the picture above and my actual implementation are:</p>
<ol>
<li>My implementation uses one hidden layer instead of two. 6 nodes in the hidden layer, but that can be adjusted easily.</li>
<li>The dataset provided is a multi-class classification problem. That makes the output layer a 3-node one instead of one.</li>
<li>In my implementation I applied <strong>tanh</strong> to the output layer.</li>
</ol>
<h2 id="pseudo-backprop">Pseudo Backprop</h2>
<p>Randomly initialize all weights<br />
for t = 0 to T (I set T to 50000 in my implementation)</p>
<ol>
<li>stochastic: randomly pick a data point from x_train</li>
<li>forward: compute all <strong>x</strong> and <strong>s</strong> by multiplying <strong>x</strong> and <strong>w</strong>ij and feeding the results to <strong>tanh</strong></li>
<li>backward: compute all the delta. I'll give the detail of delta later.</li>
<li>gradient descent: <img src="gd_pic.png" alt="gd_formular" />. l indicates the layer given l = 0 for the input layer.</li>
<li>After T iterations, we have the optimized <strong>w's</strong>. Use them to do one forward propagation for the testing set. Compute the accuracy.</li>
</ol>
<h2 id="gradient-descent">Gradient Descent</h2>
<ul>
<li>Since this is not a challenge on Gradient Descent, I won't go into details. I don't remember it anyway :)</li>
<li>What is <strong>eta</strong>? eta is the character looks like n. It represents the step size you take in the GD process.</li>
<li>What is <strong>delta</strong>?<br />
In order to apply GD, we need the partial derivative of the error term. In the picture below L is the output layer, which is 2 in our case. In the 3-class classification setting, there are 3 <strong>delta's</strong> for the output layer. And in my implementation, I applied the <strong>tanh</strong> to the output layer. So for the delta's of the output layer, we need to multiply an extra tanh.<br />
<img src="gd_partial.png" alt="gd_partial" /></li>
<li>How to compute <strong>delta</strong><br />
<img src="middle_delta_compute.png" alt="delta_compute" /></li>
</ul>
<h2 id="miscellaneous">Miscellaneous</h2>
<ul>
<li>Pictures are from the notes of a Machine Learning course I took 2.5 years ago. I implemented a basic NNet algorithm for binary classification in that class with MATLAB. This Ruby version is a modification of that implementation.</li>
<li>With the current parameters setting, after a few runs the lowest accuracy achieved is 0.96. I think the provided test set is too small to tell the true accuracy of the setting in practice. The size of the test set is 75.</li>
<li>This algorithm can be parallelized. If you know parallel programming in Ruby, please tell me how to adapt my code to do so.</li>
<li>I wrote some code to tranform the raw predicting results to the standard form of 1's and 0's. Later I found that the provided confusion matrix method didn't care. So I commented it out.</li>
</ul>
</body>
</html>
